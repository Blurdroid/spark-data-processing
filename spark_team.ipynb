{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229e98fc-5069-44ab-9532-637db330d8e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ziad-Lenovo-Legion-Y530-15ICH:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70f32e22ce30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afdebbe-3d81-4a4f-98ae-c02214988c63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**the problem statement:**\n",
    "\n",
    "We have three data tables which are of type CSV. We are performing basic joins in those tables and Creating a Demoralized data frame so that we could perform some processing and Analytics on top of our Data\n",
    "\n",
    "**the dataset :**\n",
    "\n",
    "consist of three tables customer, orders, and order_items.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f8cc1-396c-478e-ba1c-7d3d1569ab3c",
   "metadata": {},
   "source": [
    "**first : setting up the project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dff637b5-9ca1-432f-8dd4-a05bd9802347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d333e38-98d0-4ba7-9b93-04e42cd27aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, FloatType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"spark_project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dfa97d0-be0e-4cd6-aa21-33772464b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_customer = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"password\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"customer_status\", StringType(), True),\n",
    "    StructField(\"customer_zipcode\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5f314c2-c470-490e-896f-f8ac6106b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = spark.read.csv('/home/ziad/Downloads/spark_project/Data/customers/part-00000',header=True, schema=schema_customer, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce3ab577-ab13-4d90-8bc9-a78566186009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+---------+---------+--------------------+----------+---------------+----------------+\n",
      "|customer_id|first_name|last_name|    email| password|             address|      city|customer_status|customer_zipcode|\n",
      "+-----------+----------+---------+---------+---------+--------------------+----------+---------------+----------------+\n",
      "|          2|      Mary|  Barrett|XXXXXXXXX|XXXXXXXXX|9526 Noble Embers...| Littleton|             CO|           80126|\n",
      "|          3|       Ann|    Smith|XXXXXXXXX|XXXXXXXXX|3422 Blue Pioneer...|    Caguas|             PR|             725|\n",
      "|          4|      Mary|    Jones|XXXXXXXXX|XXXXXXXXX|  8324 Little Common|San Marcos|             CA|           92069|\n",
      "|          5|    Robert|   Hudson|XXXXXXXXX|XXXXXXXXX|10 Crystal River ...|    Caguas|             PR|             725|\n",
      "|          6|      Mary|    Smith|XXXXXXXXX|XXXXXXXXX|3151 Sleepy Quail...|   Passaic|             NJ|            7055|\n",
      "+-----------+----------+---------+---------+---------+--------------------+----------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 23:59:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, Richard, Hernandez, XXXXXXXXX, XXXXXXXXX, 6303 Heather Plaza, Brownsville, TX, 78521\n",
      " Schema: customer_id, first_name, last_name, email, password, address, city, customer_status, customer_zipcode\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n"
     ]
    }
   ],
   "source": [
    "customer.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af6549c6-ad88-4fa4-8435-18d2c1c1b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_order_item = StructType([\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"order_item_order_id\", IntegerType(), True),\n",
    "    StructField(\"order_item_product_id\", IntegerType(), True),\n",
    "    StructField(\"order_item_quantity\", IntegerType(), True),\n",
    "    StructField(\"order_item_subtotal\", FloatType(), True),\n",
    "    StructField(\"order_item_product_price\", FloatType(), True)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e13dc837-b2d1-40e9-bbcf-0f852eea4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_item = spark.read.csv('/home/ziad/Downloads/spark_project/Data/order_items/part-00000',header=True, schema= schema_order_item, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "554366bb-6f0e-447c-8cb8-aae7c73325cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 23:58:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 1, 957, 1, 299.98, 299.98\n",
      " Schema: order_item_id, order_item_order_id, order_item_product_id, order_item_quantity, order_item_subtotal, order_item_product_price\n",
      "Expected: order_item_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/order_items/part-00000\n"
     ]
    }
   ],
   "source": [
    "order_item.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb03e516-437e-40f7-9f6b-36e88dd0601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_order = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", TimestampType(), True),\n",
    "    StructField(\"order_customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15b14983-105e-460e-8665-641f51d96863",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv('/home/ziad/Downloads/spark_project/Data/orders/part-00000', header=True, schema = schema_order, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cd64d870-5c86-4b85-85fc-9dadab2abdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:00:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc5cad-8397-4bab-be5a-dbf3ca327fe2",
   "metadata": {},
   "source": [
    "**second: join tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84d0e5ea-26d5-4c47-9d56-418783ed29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_orders_=customer.join(orders,customer['customer_id']==orders['order_customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c11c995-0a1d-43ce-817e-037ff6f81f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 20:25:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, Richard, Hernandez, XXXXXXXXX, XXXXXXXXX, 6303 Heather Plaza, Brownsville, TX, 78521\n",
      " Schema: customer_id, first_name, last_name, email, password, address, city, customer_status, customer_zipcode\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+---------+---------+--------------------+-----------+---------------+----------------+--------+-------------------+-----------------+---------------+\n",
      "|customer_id|first_name|last_name|    email| password|             address|       city|customer_status|customer_zipcode|order_id|         order_date|order_customer_id|   order_status|\n",
      "+-----------+----------+---------+---------+---------+--------------------+-----------+---------------+----------------+--------+-------------------+-----------------+---------------+\n",
      "|        256|     David|Rodriguez|XXXXXXXXX|XXXXXXXXX|7605 Tawny Horse ...|    Chicago|             IL|           60625|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|      12111|     Amber|   Franco|XXXXXXXXX|XXXXXXXXX|8766 Clear Prairi...| Santa Cruz|             CA|           95060|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       8827|     Brian|   Wilson|XXXXXXXXX|XXXXXXXXX|   8396 High Corners|San Antonio|             TX|           78240|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|      11318|      Mary|    Henry|XXXXXXXXX|XXXXXXXXX|3047 Silent Ember...|     Caguas|             PR|             725|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       7130|     Alice|    Smith|XXXXXXXXX|XXXXXXXXX|      8852 Iron Port|   Brooklyn|             NY|           11237|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "+-----------+----------+---------+---------+---------+--------------------+-----------+---------------+----------------+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 20:25:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "customers_orders_.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d11adc6b-0d81-4946-9eca-43436f44c934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 20:27:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1\n",
      " Schema: customer_id\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n",
      "25/03/08 20:27:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------------+---------------+\n",
      "|customer_id|order_id|         order_date|   order_status|\n",
      "+-----------+--------+-------------------+---------------+\n",
      "|          2|   33865|2014-02-18 00:00:00|       COMPLETE|\n",
      "|          2|   15192|2013-10-29 00:00:00|PENDING_PAYMENT|\n",
      "|          2|   57963|2013-08-02 00:00:00|        ON_HOLD|\n",
      "|          2|   67863|2013-11-30 00:00:00|       COMPLETE|\n",
      "|          3|   22646|2013-12-11 00:00:00|       COMPLETE|\n",
      "|          3|   23662|2013-12-19 00:00:00|       COMPLETE|\n",
      "|          3|   56178|2014-07-15 00:00:00|        PENDING|\n",
      "|          3|   35158|2014-02-26 00:00:00|       COMPLETE|\n",
      "|          3|   46399|2014-05-09 00:00:00|     PROCESSING|\n",
      "|          3|   57617|2014-07-24 00:00:00|       COMPLETE|\n",
      "+-----------+--------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_orders_.select('customer_id','order_id','order_date','order_status').orderBy('customer_id').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c207aae-7d30-4a13-8bf4-0d5c0131372a",
   "metadata": {},
   "source": [
    "**Consolidating order_id,order_date, and order_status to structure data type.**\n",
    "-->nested column that combines multiple columns into a single struct (complex) type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fd6038a-07e9-442c-8674-74668f281559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "customer_order_struct=customers_orders_.select('customer_id',struct('order_id','order_date','order_status').alias('order_details'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7dae21f9-8ee9-4af7-94ba-789d2710ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 20:40:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1\n",
      " Schema: customer_id\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n",
      "25/03/08 20:40:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------------+\n",
      "|customer_id|order_details                            |\n",
      "+-----------+-----------------------------------------+\n",
      "|256        |{2, 2013-07-25 00:00:00, PENDING_PAYMENT}|\n",
      "|12111      |{3, 2013-07-25 00:00:00, COMPLETE}       |\n",
      "|8827       |{4, 2013-07-25 00:00:00, CLOSED}         |\n",
      "|11318      |{5, 2013-07-25 00:00:00, COMPLETE}       |\n",
      "|7130       |{6, 2013-07-25 00:00:00, COMPLETE}       |\n",
      "+-----------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_order_struct.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056e814-d768-42ff-96e2-63feffb9b71f",
   "metadata": {},
   "source": [
    "**denormalization of customer and order table into one table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebedec85-415f-41cc-b63a-964e8818fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=customer_order_struct.groupBy('customer_id').agg(collect_list('order_details').alias('order_details')).orderBy('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bcb256fe-b997-44f2-bdd2-0471b63c3006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 20:41:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1\n",
      " Schema: customer_id\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n",
      "25/03/08 20:41:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n",
      "[Stage 35:==========================================================(1 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|customer_id|order_details                                                                                                                                                                                                                                                                            |\n",
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2          |[{15192, 2013-10-29 00:00:00, PENDING_PAYMENT}, {33865, 2014-02-18 00:00:00, COMPLETE}, {57963, 2013-08-02 00:00:00, ON_HOLD}, {67863, 2013-11-30 00:00:00, COMPLETE}]                                                                                                                   |\n",
      "|3          |[{22646, 2013-12-11 00:00:00, COMPLETE}, {23662, 2013-12-19 00:00:00, COMPLETE}, {35158, 2014-02-26 00:00:00, COMPLETE}, {46399, 2014-05-09 00:00:00, PROCESSING}, {56178, 2014-07-15 00:00:00, PENDING}, {57617, 2014-07-24 00:00:00, COMPLETE}, {61453, 2013-12-14 00:00:00, COMPLETE}]|\n",
      "|4          |[{9023, 2013-09-19 00:00:00, COMPLETE}, {9704, 2013-09-24 00:00:00, COMPLETE}, {17253, 2013-11-09 00:00:00, PENDING_PAYMENT}, {37878, 2014-03-15 00:00:00, COMPLETE}, {49339, 2014-05-28 00:00:00, COMPLETE}, {51157, 2014-06-10 00:00:00, CLOSED}]                                      |\n",
      "|5          |[{13705, 2013-10-18 00:00:00, COMPLETE}, {36472, 2014-03-06 00:00:00, PROCESSING}, {41333, 2014-04-05 00:00:00, COMPLETE}, {45832, 2014-05-05 00:00:00, PENDING_PAYMENT}]                                                                                                                |\n",
      "|6          |[{7485, 2013-09-09 00:00:00, PROCESSING}, {7787, 2013-09-10 00:00:00, PENDING}, {22457, 2013-12-10 00:00:00, PENDING_PAYMENT}, {32895, 2014-02-13 00:00:00, PENDING_PAYMENT}]                                                                                                            |\n",
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b0b2e13-4151-4b79-84fd-2b9fc88bac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/08 20:44:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1\n",
      " Schema: customer_id\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n",
      "25/03/08 20:44:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "final_df.coalesce(1).write.mode(\"overwrite\").json(\"/home/ziad/Downloads/spark_project/Data/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d61f5-eeb6-4031-b6f9-a40cac5ef12c",
   "metadata": {},
   "source": [
    "**join customer with order_item**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78d6db6e-27b8-4550-b479-195e5fcf2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_details=customer. \\\n",
    "join(orders,customer['customer_id']==orders['order_customer_id']). \\\n",
    "join(order_item,orders['order_id']==order_item['order_item_order_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "865eb7e6-199a-442b-88b1-62075622c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_details.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567d57c-83a7-43ab-a713-7118288e7645",
   "metadata": {},
   "source": [
    "####  Difference Between `collect_list()` and `struct()` in PySpark  \n",
    "\n",
    "###### ðŸ”¹ `collect_list()`: Aggregates Data into a List  \n",
    "- Groups rows based on a key and **collects multiple values** into a list.  \n",
    "\n",
    "###### ðŸ”¹ `struct()`: Combines multiple columns into a single structured column.  \n",
    "- Combines multiple columns into a single structured column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652124d-e497-44ec-b5e1-c1ff2801e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c7a25e8-49e9-40d8-bf80-6098f9a8575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_df=customer_details. \\\n",
    "select('customer_id','first_name','last_name','email','order_id','order_date','order_status',struct('order_item_id','order_item_product_id','order_item_subtotal').alias('order_item_details')). \\\n",
    "groupBy('customer_id','first_name','last_name','email','order_id','order_date','order_status'). \\\n",
    "agg(collect_list('order_item_details').alias('order_item_details')). \\\n",
    "orderBy('customer_id').\\\n",
    "select('customer_id','first_name','last_name','email',struct('order_id','order_date','order_status','order_item_details').alias('order_details')). \\\n",
    "groupBy('customer_id','first_name','last_name','email'). \\\n",
    "agg(collect_list('order_details').alias('order_details')). \\\n",
    "orderBy('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef3a003f-abbb-4367-acc4-522f98d0cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:02:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, Richard, Hernandez, XXXXXXXXX\n",
      " Schema: customer_id, first_name, last_name, email\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n",
      "25/03/09 00:02:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 1, 957, 299.98\n",
      " Schema: order_item_id, order_item_order_id, order_item_product_id, order_item_subtotal\n",
      "Expected: order_item_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/order_items/part-00000\n",
      "25/03/09 00:02:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|customer_id|first_name |last_name|email    |order_details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------+-----------+---------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2          |Mary       |Barrett  |XXXXXXXXX|[{67863, 2013-11-30 00:00:00, COMPLETE, [{169674, 1004, 399.98}]}, {15192, 2013-10-29 00:00:00, PENDING_PAYMENT, [{38007, 1014, 99.96}]}, {33865, 2014-02-18 00:00:00, COMPLETE, [{84538, 502, 50.0}, {84537, 1073, 199.99}, {84536, 957, 299.98}]}, {57963, 2013-08-02 00:00:00, ON_HOLD, [{145023, 1014, 149.94}, {145022, 1014, 99.96}, {145021, 627, 199.95}, {145020, 1073, 199.99}, {145019, 365, 119.98}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|3          |Ann        |Smith    |XXXXXXXXX|[{61453, 2013-12-14 00:00:00, COMPLETE, [{153690, 1073, 199.99}, {153689, 957, 299.98}, {153688, 957, 299.98}, {153687, 403, 129.99}]}, {35158, 2014-02-26 00:00:00, COMPLETE, [{87813, 273, 27.99}, {87812, 1004, 399.98}]}, {23662, 2013-12-19 00:00:00, COMPLETE, [{59209, 502, 50.0}, {59208, 502, 100.0}, {59207, 191, 99.99}]}, {56178, 2014-07-15 00:00:00, PENDING, [{140510, 957, 299.98}, {140509, 502, 150.0}, {140508, 957, 299.98}, {140507, 365, 299.95}, {140506, 502, 100.0}]}, {57617, 2014-07-24 00:00:00, COMPLETE, [{144132, 1073, 199.99}, {144131, 1014, 99.96}, {144130, 365, 239.96}, {144129, 365, 239.96}]}]                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|4          |Mary       |Jones    |XXXXXXXXX|[{9023, 2013-09-19 00:00:00, COMPLETE, [{22498, 1014, 149.94}, {22497, 885, 24.99}, {22496, 627, 79.98}, {22495, 1014, 99.96}]}, {51157, 2014-06-10 00:00:00, CLOSED, [{127853, 365, 59.99}, {127852, 365, 119.98}, {127851, 365, 119.98}]}, {9704, 2013-09-24 00:00:00, COMPLETE, [{24241, 365, 179.97}, {24240, 365, 59.99}, {24239, 905, 124.95}, {24238, 365, 179.97}]}, {49339, 2014-05-28 00:00:00, COMPLETE, [{123340, 365, 299.95}, {123339, 365, 119.98}, {123338, 502, 100.0}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|5          |Robert     |Hudson   |XXXXXXXXX|[{45832, 2014-05-05 00:00:00, PENDING_PAYMENT, [{114561, 365, 239.96}]}, {41333, 2014-04-05 00:00:00, COMPLETE, [{103183, 403, 129.99}]}, {36472, 2014-03-06 00:00:00, PROCESSING, [{91068, 957, 299.98}, {91067, 235, 174.95}, {91066, 1014, 199.92}, {91065, 403, 129.99}, {91064, 1014, 99.96}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|6          |Mary       |Smith    |XXXXXXXXX|[{7787, 2013-09-10 00:00:00, PENDING, [{19475, 365, 179.97}, {19474, 627, 199.95}, {19473, 1014, 249.9}]}, {22457, 2013-12-10 00:00:00, PENDING_PAYMENT, [{56204, 1073, 199.99}, {56203, 403, 129.99}, {56202, 1004, 399.98}, {56201, 1014, 99.96}, {56200, 1014, 99.96}]}, {32895, 2014-02-13 00:00:00, PENDING_PAYMENT, [{82281, 502, 250.0}, {82280, 502, 100.0}, {82279, 191, 299.97}]}, {7485, 2013-09-09 00:00:00, PROCESSING, [{18769, 1014, 149.94}, {18768, 191, 499.95}, {18767, 502, 200.0}, {18766, 627, 199.95}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|7          |Melissa    |Wilcox   |XXXXXXXXX|[{9977, 2013-09-25 00:00:00, PROCESSING, [{24928, 502, 150.0}, {24927, 502, 150.0}]}, {26730, 2014-01-06 00:00:00, ON_HOLD, [{66951, 502, 150.0}, {66950, 191, 499.95}, {66949, 1004, 399.98}, {66948, 191, 199.98}]}, {35559, 2014-03-01 00:00:00, PROCESSING, [{88802, 191, 199.98}, {88801, 1014, 249.9}, {88800, 897, 49.98}]}, {62132, 2014-01-09 00:00:00, ON_HOLD, [{155343, 1004, 399.98}, {155342, 957, 299.98}, {155341, 403, 129.99}, {155340, 957, 299.98}, {155339, 502, 100.0}]}, {26052, 2014-01-02 00:00:00, PENDING_PAYMENT, [{65203, 1014, 199.92}, {65202, 1004, 399.98}, {65201, 1004, 399.98}, {65200, 403, 129.99}]}, {28539, 2014-01-17 00:00:00, PENDING_PAYMENT, [{71414, 403, 129.99}, {71413, 1004, 399.98}]}, {61683, 2013-12-22 00:00:00, COMPLETE, [{154218, 191, 499.95}, {154217, 403, 129.99}]}]                                                                                                                                                                                                       |\n",
      "|8          |Megan      |Smith    |XXXXXXXXX|[{19801, 2013-11-24 00:00:00, PENDING, [{49487, 191, 499.95}]}, {62064, 2014-01-08 00:00:00, PENDING_PAYMENT, [{155181, 502, 150.0}]}, {68507, 2014-05-30 00:00:00, PENDING, [{171306, 1073, 199.99}, {171305, 365, 239.96}]}, {7688, 2013-09-10 00:00:00, PROCESSING, [{19227, 365, 59.99}, {19226, 502, 200.0}]}, {29383, 2014-01-22 00:00:00, COMPLETE, [{73519, 917, 109.95}, {73518, 502, 200.0}, {73517, 917, 43.98}]}, {8497, 2013-09-16 00:00:00, CLOSED, [{21219, 502, 250.0}, {21218, 1014, 249.9}, {21217, 1014, 149.94}, {21216, 1014, 199.92}]}, {22297, 2013-12-08 00:00:00, COMPLETE, [{55831, 365, 59.99}, {55830, 191, 199.98}, {55829, 502, 150.0}, {55828, 1073, 199.99}, {55827, 957, 299.98}]}, {65018, 2014-04-27 00:00:00, PENDING_PAYMENT, [{162522, 957, 299.98}]}]                                                                                                                                                                                                                                            |\n",
      "|9          |Mary       |Perez    |XXXXXXXXX|[{12828, 2013-10-12 00:00:00, COMPLETE, [{32074, 906, 24.99}, {32073, 627, 199.95}]}, {64753, 2014-04-17 00:00:00, PENDING_PAYMENT, [{161850, 1004, 399.98}]}, {54350, 2014-07-03 00:00:00, CLOSED, [{135917, 1004, 399.98}, {135916, 37, 174.95}, {135915, 403, 129.99}]}, {18918, 2013-11-19 00:00:00, PENDING_PAYMENT, [{47314, 502, 100.0}, {47313, 365, 299.95}, {47312, 1014, 99.96}, {47311, 957, 299.98}, {47310, 1073, 199.99}]}, {67610, 2013-09-12 00:00:00, PENDING_PAYMENT, [{169042, 1004, 399.98}, {169041, 364, 299.99}, {169040, 1073, 199.99}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|10         |Melissa    |Smith    |XXXXXXXXX|[{56133, 2014-07-15 00:00:00, COMPLETE, [{140389, 1014, 149.94}, {140388, 1014, 99.96}, {140387, 627, 39.99}, {140386, 897, 124.95}]}, {45239, 2014-05-01 00:00:00, COMPLETE, [{113029, 502, 150.0}, {113028, 365, 299.95}, {113027, 502, 200.0}, {113026, 502, 200.0}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|11         |Mary       |Huffman  |XXXXXXXXX|[{42837, 2014-04-16 00:00:00, PENDING_PAYMENT, [{106967, 1073, 199.99}, {106966, 365, 119.98}]}, {11811, 2013-10-05 00:00:00, COMPLETE, [{29538, 771, 199.95}, {29537, 365, 239.96}, {29536, 365, 119.98}, {29535, 1014, 249.9}]}, {17625, 2013-11-11 00:00:00, CLOSED, [{44052, 365, 119.98}, {44051, 365, 299.95}, {44050, 1004, 399.98}]}, {51781, 2014-06-14 00:00:00, COMPLETE, [{129372, 403, 129.99}, {129371, 191, 99.99}, {129370, 502, 250.0}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|12         |Christopher|Smith    |XXXXXXXXX|[{20537, 2013-11-29 00:00:00, PENDING_PAYMENT, [{51293, 957, 299.98}]}, {921, 2013-07-30 00:00:00, PENDING_PAYMENT, [{2303, 1014, 199.92}, {2302, 502, 50.0}]}, {59731, 2013-10-10 00:00:00, CLOSED, [{149470, 365, 179.97}, {149469, 1004, 399.98}, {149468, 1073, 199.99}, {149467, 1014, 49.98}]}, {24642, 2013-12-25 00:00:00, PAYMENT_REVIEW, [{61698, 365, 299.95}, {61697, 957, 299.98}, {61696, 627, 199.95}, {61695, 1073, 199.99}, {61694, 1004, 399.98}]}, {36246, 2014-03-04 00:00:00, PENDING_PAYMENT, [{90511, 365, 239.96}, {90510, 502, 150.0}, {90509, 403, 129.99}, {90508, 1073, 199.99}, {90507, 403, 129.99}]}, {45831, 2014-05-05 00:00:00, PENDING_PAYMENT, [{114560, 893, 99.96}, {114559, 502, 150.0}, {114558, 1073, 199.99}, {114557, 191, 199.98}, {114556, 1073, 199.99}]}, {44131, 2014-04-25 00:00:00, PROCESSING, [{110201, 191, 499.95}, {110200, 191, 399.96}, {110199, 1073, 199.99}, {110198, 191, 199.98}, {110197, 1014, 99.96}]}, {58919, 2013-09-07 00:00:00, ON_HOLD, [{147487, 403, 129.99}]}]|\n",
      "|13         |Mary       |Baldwin  |XXXXXXXXX|[{22725, 2013-12-11 00:00:00, COMPLETE, [{56872, 1014, 249.9}]}, {63963, 2014-03-18 00:00:00, ON_HOLD, [{159911, 365, 119.98}, {159910, 365, 59.99}, {159909, 652, 129.99}, {159908, 502, 50.0}, {159907, 1014, 149.94}]}, {36217, 2014-03-04 00:00:00, COMPLETE, [{90436, 957, 299.98}, {90435, 502, 250.0}, {90434, 365, 179.97}, {90433, 191, 499.95}, {90432, 403, 129.99}]}, {2109, 2013-08-05 00:00:00, COMPLETE, [{5283, 957, 299.98}, {5282, 502, 50.0}, {5281, 1004, 399.98}, {5280, 957, 299.98}, {5279, 926, 15.99}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|14         |Katherine  |Smith    |XXXXXXXXX|[{39208, 2014-03-23 00:00:00, ON_HOLD, [{97872, 191, 399.96}]}, {9453, 2013-09-22 00:00:00, COMPLETE, [{23611, 502, 200.0}, {23610, 957, 299.98}]}, {21077, 2013-12-01 00:00:00, CLOSED, [{52702, 1014, 249.9}]}, {58343, 2013-08-17 00:00:00, PROCESSING, [{146012, 403, 129.99}, {146011, 1073, 199.99}, {146010, 403, 129.99}]}, {26976, 2014-01-08 00:00:00, COMPLETE, [{67571, 1073, 199.99}, {67570, 365, 59.99}, {67569, 1004, 399.98}, {67568, 793, 44.97}]}, {63552, 2014-03-02 00:00:00, PENDING, [{158865, 1014, 49.98}]}, {66343, 2014-06-15 00:00:00, PENDING_PAYMENT, [{165840, 502, 50.0}]}]                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|15         |Jane       |Luna     |XXXXXXXXX|[{48376, 2014-05-21 00:00:00, COMPLETE, [{120997, 502, 50.0}, {120996, 403, 129.99}, {120995, 365, 59.99}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|16         |Tiffany    |Smith    |XXXXXXXXX|[{191, 2013-07-26 00:00:00, CLOSED, [{465, 627, 39.99}]}, {43399, 2014-04-20 00:00:00, COMPLETE, [{108399, 1014, 49.98}, {108398, 403, 129.99}, {108397, 502, 100.0}]}, {34997, 2014-02-25 00:00:00, COMPLETE, [{87423, 1014, 49.98}]}, {41831, 2014-04-09 00:00:00, SUSPECTED_FRAUD, [{104423, 1014, 99.96}, {104422, 365, 179.97}]}, {19211, 2013-11-21 00:00:00, PENDING_PAYMENT, [{47994, 1073, 199.99}, {47993, 502, 250.0}, {47992, 502, 50.0}, {47991, 1073, 199.99}, {47990, 191, 199.98}]}, {68759, 2013-11-27 00:00:00, COMPLETE, [{171901, 1073, 199.99}, {171900, 1014, 49.98}, {171899, 502, 150.0}, {171898, 957, 299.98}, {171897, 191, 99.99}]}]                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|17         |Mary       |Robinson |XXXXXXXXX|[{789, 2013-07-29 00:00:00, COMPLETE, [{1949, 957, 299.98}, {1948, 1004, 399.98}]}, {30338, 2014-01-29 00:00:00, CLOSED, [{75838, 957, 299.98}, {75837, 502, 150.0}, {75836, 627, 119.97}]}, {17053, 2013-11-08 00:00:00, ON_HOLD, [{42643, 957, 299.98}, {42642, 191, 199.98}]}, {52604, 2014-06-19 00:00:00, COMPLETE, [{131488, 728, 325.0}, {131487, 134, 25.0}, {131486, 1004, 399.98}, {131485, 403, 129.99}, {131484, 1004, 399.98}]}, {57733, 2014-07-24 00:00:00, CLOSED, [{144433, 957, 299.98}, {144432, 1014, 149.94}, {144431, 957, 299.98}, {144430, 627, 159.96}, {144429, 957, 299.98}]}]                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|18         |Robert     |Smith    |XXXXXXXXX|[{3742, 2013-08-15 00:00:00, COMPLETE, [{9305, 403, 129.99}]}, {19709, 2013-11-24 00:00:00, CANCELED, [{49277, 502, 100.0}, {49276, 1014, 199.92}, {49275, 1004, 399.98}]}, {27740, 2014-01-12 00:00:00, COMPLETE, [{69460, 1004, 399.98}, {69459, 403, 129.99}, {69458, 1014, 149.94}]}, {49411, 2014-05-29 00:00:00, COMPLETE, [{123507, 957, 299.98}]}, {6912, 2013-09-06 00:00:00, CLOSED, [{17308, 191, 399.96}, {17307, 627, 119.97}, {17306, 191, 299.97}, {17305, 403, 129.99}, {17304, 403, 129.99}]}, {27805, 2014-01-13 00:00:00, COMPLETE, [{69610, 403, 129.99}, {69609, 191, 499.95}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|19         |Stephanie  |Mitchell |XXXXXXXXX|[{1018, 2013-07-30 00:00:00, COMPLETE, [{2542, 403, 129.99}, {2541, 1004, 399.98}, {2540, 502, 150.0}]}, {32079, 2014-02-08 00:00:00, PENDING, [{80221, 1004, 399.98}]}, {12181, 2013-10-07 00:00:00, COMPLETE, [{30454, 822, 95.98}, {30453, 957, 299.98}, {30452, 403, 129.99}]}, {16447, 2013-11-05 00:00:00, CLOSED, [{41069, 502, 100.0}, {41068, 278, 179.96}, {41067, 502, 200.0}]}, {37228, 2014-03-11 00:00:00, CLOSED, [{92918, 403, 129.99}, {92917, 1073, 199.99}, {92916, 365, 119.98}]}, {6122, 2013-09-01 00:00:00, CLOSED, [{15305, 191, 399.96}, {15304, 403, 129.99}, {15303, 365, 59.99}, {15302, 502, 200.0}, {15301, 1014, 49.98}]}, {21640, 2013-12-05 00:00:00, COMPLETE, [{54110, 365, 179.97}]}, {45390, 2014-05-02 00:00:00, PENDING_PAYMENT, [{113423, 403, 129.99}, {113422, 502, 250.0}, {113421, 365, 179.97}, {113420, 627, 119.97}, {113419, 365, 119.98}]}]                                                                                                                                            |\n",
      "|20         |Mary       |Ellis    |XXXXXXXXX|[{21533, 2013-12-04 00:00:00, PENDING_PAYMENT, [{53836, 403, 129.99}, {53835, 627, 119.97}]}, {57269, 2014-07-21 00:00:00, PROCESSING, [{143253, 365, 299.95}]}, {5101, 2013-08-25 00:00:00, COMPLETE, [{12755, 191, 499.95}, {12754, 365, 239.96}]}, {15199, 2013-10-29 00:00:00, COMPLETE, [{38017, 365, 299.95}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|21         |William    |Zimmerman|XXXXXXXXX|[{33481, 2014-02-16 00:00:00, PENDING, [{83599, 403, 129.99}, {83598, 906, 74.97}, {83597, 821, 51.99}, {83596, 1073, 199.99}]}, {56977, 2014-07-20 00:00:00, PROCESSING, [{142513, 1014, 149.94}, {142512, 403, 129.99}, {142511, 403, 129.99}, {142510, 365, 59.99}]}, {25802, 2013-12-31 00:00:00, PENDING_PAYMENT, [{64591, 502, 250.0}, {64590, 1014, 149.94}, {64589, 403, 129.99}, {64588, 365, 299.95}]}, {16646, 2013-11-06 00:00:00, COMPLETE, [{41598, 502, 200.0}, {41597, 172, 150.0}, {41596, 403, 129.99}, {41595, 1073, 199.99}, {41594, 1004, 399.98}]}]                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-----------+-----------+---------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "denorm_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aecbe2-bb53-42eb-8628-f37581b0a292",
   "metadata": {},
   "source": [
    "**summary**\n",
    "- This query denormalizes the customer, order, and order-item details by:\n",
    "\n",
    "Structuring order items into order_item_details (per order).\n",
    "Grouping order items using collect_list(), so each order has all its items in a list.\n",
    "Structuring order details into order_details (per customer).\n",
    "Grouping all orders per customer using collect_list(), so each customer has all their orders in a single row.\n",
    "Sorting the final result by customer_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a4e81e5-2764-447b-9b5e-0a2f6c5c1980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- order_details: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- order_id: integer (nullable = true)\n",
      " |    |    |-- order_date: timestamp (nullable = true)\n",
      " |    |    |-- order_status: string (nullable = true)\n",
      " |    |    |-- order_item_details: array (nullable = false)\n",
      " |    |    |    |-- element: struct (containsNull = false)\n",
      " |    |    |    |    |-- order_item_id: integer (nullable = true)\n",
      " |    |    |    |    |-- order_item_product_id: integer (nullable = true)\n",
      " |    |    |    |    |-- order_item_subtotal: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "denorm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e2ceb254-8ee4-4f4c-881b-af47db446803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:04:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, Richard, Hernandez, XXXXXXXXX\n",
      " Schema: customer_id, first_name, last_name, email\n",
      "Expected: customer_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/customers/part-00000\n",
      "25/03/09 00:04:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 1, 957, 299.98\n",
      " Schema: order_item_id, order_item_order_id, order_item_product_id, order_item_subtotal\n",
      "Expected: order_item_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/order_items/part-00000\n",
      "25/03/09 00:04:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 2013-07-25 00:00:00.0, 11599, CLOSED\n",
      " Schema: order_id, order_date, order_customer_id, order_status\n",
      "Expected: order_id but found: 1\n",
      "CSV file: file:///home/ziad/Downloads/spark_project/Data/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "denorm_df.coalesce(1).write.mode('overwrite').json('/home/ziad/Downloads/spark_project/Data/output_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "94f90cdc-445a-422e-8e43-5becdd31e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df=spark.read.json('/home/ziad/Downloads/spark_project/Data/output_1/part-00000-7c628991-def1-4852-bb82-77beeb9b3cf2-c000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9a3e8e7-cb7e-4192-b9e3-9b9211df7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------+---------+--------------------+\n",
      "|customer_id|    email| first_name|last_name|       order_details|\n",
      "+-----------+---------+-----------+---------+--------------------+\n",
      "|          2|XXXXXXXXX|       Mary|  Barrett|[{2013-11-30T00:0...|\n",
      "|          3|XXXXXXXXX|        Ann|    Smith|[{2013-12-14T00:0...|\n",
      "|          4|XXXXXXXXX|       Mary|    Jones|[{2013-09-19T00:0...|\n",
      "|          5|XXXXXXXXX|     Robert|   Hudson|[{2014-05-05T00:0...|\n",
      "|          6|XXXXXXXXX|       Mary|    Smith|[{2013-09-10T00:0...|\n",
      "|          7|XXXXXXXXX|    Melissa|   Wilcox|[{2013-09-25T00:0...|\n",
      "|          8|XXXXXXXXX|      Megan|    Smith|[{2013-11-24T00:0...|\n",
      "|          9|XXXXXXXXX|       Mary|    Perez|[{2013-10-12T00:0...|\n",
      "|         10|XXXXXXXXX|    Melissa|    Smith|[{2014-07-15T00:0...|\n",
      "|         11|XXXXXXXXX|       Mary|  Huffman|[{2014-04-16T00:0...|\n",
      "|         12|XXXXXXXXX|Christopher|    Smith|[{2013-11-29T00:0...|\n",
      "|         13|XXXXXXXXX|       Mary|  Baldwin|[{2013-12-11T00:0...|\n",
      "|         14|XXXXXXXXX|  Katherine|    Smith|[{2014-03-23T00:0...|\n",
      "|         15|XXXXXXXXX|       Jane|     Luna|[{2014-05-21T00:0...|\n",
      "|         16|XXXXXXXXX|    Tiffany|    Smith|[{2013-07-26T00:0...|\n",
      "|         17|XXXXXXXXX|       Mary| Robinson|[{2013-07-29T00:0...|\n",
      "|         18|XXXXXXXXX|     Robert|    Smith|[{2013-08-15T00:0...|\n",
      "|         19|XXXXXXXXX|  Stephanie| Mitchell|[{2013-07-30T00:0...|\n",
      "|         20|XXXXXXXXX|       Mary|    Ellis|[{2013-12-04T00:0...|\n",
      "|         21|XXXXXXXXX|    William|Zimmerman|[{2014-02-16T00:0...|\n",
      "+-----------+---------+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "937f6bde-bf37-4ad9-b1d3-091fa68e11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,filter, col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd10ff-8d63-45f5-8a32-17ec6d11c64b",
   "metadata": {},
   "source": [
    "**problem statement1**\n",
    "- Ø¹Ø§ÙŠØ²ÙŠÙ† Ø§Ù„ Ø¯ÙŠØª ÙŠÙƒÙˆÙ† 2014 Ø¨ØªØ§Ø¹ Ø§Ù„ÙƒØ§Ø³ØªÙ…Ø± ÙˆØ§Ù„Ø³ØªØ§ØªÙŠØ³  ÙˆÙƒÙ…Ø§Ù† Ø§Ù„ Ø§ÙŠ Ø¯ÙŠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d49c300d-df39-49b7-8cdb-a21dfd035088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+--------------------+---------------+\n",
      "|customer_id|first_name|order_id|          order_date|   order_status|\n",
      "+-----------+----------+--------+--------------------+---------------+\n",
      "|        206|      Mary|   25966|2014-01-01T00:00:...|         CLOSED|\n",
      "|        279|      Anna|   25918|2014-01-01T00:00:...|       COMPLETE|\n",
      "|        363|  Jennifer|   25980|2014-01-01T00:00:...|       COMPLETE|\n",
      "|        387|      Mary|   25970|2014-01-01T00:00:...|       COMPLETE|\n",
      "|        470|      Mary|   61904|2014-01-01T00:00:...|       COMPLETE|\n",
      "|        492|      Mary|   25964|2014-01-01T00:00:...|     PROCESSING|\n",
      "|        505|      Mary|   25925|2014-01-01T00:00:...|PENDING_PAYMENT|\n",
      "|        522|   William|   25972|2014-01-01T00:00:...|PENDING_PAYMENT|\n",
      "|       1044|     Linda|   25895|2014-01-01T00:00:...|       COMPLETE|\n",
      "|       1117|    Arthur|   25942|2014-01-01T00:00:...|     PROCESSING|\n",
      "+-----------+----------+--------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json_df.select('customer_id','first_name',explode('order_details').alias('order_details')). \\\n",
    "filter('order_details.order_date LIKE \"2014-01-01%\"'). \\\n",
    "orderBy('customer_id'). \\\n",
    "select('customer_id','first_name','order_details.order_id','order_details.order_date','order_details.order_status'). \\\n",
    "show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a596883-4ae8-4da4-9a03-f5204bd0e71e",
   "metadata": {},
   "source": [
    "**problem statement2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f233e67c-37b3-4828-8d19-71e148bbdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten=json_df.select('customer_id','first_name',explode('order_details').alias('order_details')). \\\n",
    "select('customer_id','first_name',col('order_details.order_date').alias('order_date'),col('order_details.order_id').alias('order_id'),col('order_details.order_status').alias('order_status'),explode('order_details.order_item_details').alias('order_item_details')). \\\n",
    "select('customer_id','first_name','order_date','order_id','order_status','order_item_details.order_item_id','order_item_details.order_item_product_id','order_item_details.order_item_subtotal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4631eaa2-84fb-4a8f-858d-b0b35a4f2c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+--------+---------------+-------------+---------------------+-------------------+\n",
      "|customer_id|first_name|          order_date|order_id|   order_status|order_item_id|order_item_product_id|order_item_subtotal|\n",
      "+-----------+----------+--------------------+--------+---------------+-------------+---------------------+-------------------+\n",
      "|          2|      Mary|2013-11-30T00:00:...|   67863|       COMPLETE|       169674|                 1004|             399.98|\n",
      "|          2|      Mary|2013-10-29T00:00:...|   15192|PENDING_PAYMENT|        38007|                 1014|              99.96|\n",
      "|          2|      Mary|2014-02-18T00:00:...|   33865|       COMPLETE|        84538|                  502|               50.0|\n",
      "+-----------+----------+--------------------+--------+---------------+-------------+---------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatten.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c07baed8-5a7a-404e-838d-2c1be82c587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatten.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9f7bbcb-aea9-4b2d-9977-341d242768b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.functions import date_format,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f02c34dd-4f88-4a18-b06a-7a4dc901a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 00:21:49 ERROR Executor: Exception in task 4.0 in stage 136.0 (TID 192)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-12-17T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-12-17T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "25/03/09 00:21:49 ERROR Executor: Exception in task 1.0 in stage 136.0 (TID 189)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-10-23T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-10-23T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "25/03/09 00:21:49 ERROR Executor: Exception in task 5.0 in stage 136.0 (TID 193)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-12-18T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-12-18T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "25/03/09 00:21:49 ERROR Executor: Exception in task 3.0 in stage 136.0 (TID 191)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-11-02T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-11-02T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "25/03/09 00:21:49 ERROR Executor: Exception in task 0.0 in stage 136.0 (TID 188)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-11-30T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-11-30T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "25/03/09 00:21:49 ERROR Executor: Exception in task 2.0 in stage 136.0 (TID 190)\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-09-26T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-09-26T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "25/03/09 00:21:49 WARN TaskSetManager: Lost task 3.0 in stage 136.0 (TID 191) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-11-02T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-11-02T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "\n",
      "25/03/09 00:21:49 ERROR TaskSetManager: Task 3 in stage 136.0 failed 1 times; aborting job\n",
      "25/03/09 00:21:49 WARN TaskSetManager: Lost task 5.0 in stage 136.0 (TID 193) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-12-18T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-12-18T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "\n",
      "25/03/09 00:21:49 WARN TaskSetManager: Lost task 0.0 in stage 136.0 (TID 188) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-11-30T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-11-30T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "\n",
      "25/03/09 00:21:49 WARN TaskSetManager: Lost task 1.0 in stage 136.0 (TID 189) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-10-23T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-10-23T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "\n",
      "25/03/09 00:21:49 WARN TaskSetManager: Lost task 4.0 in stage 136.0 (TID 192) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-12-17T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-12-17T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "\n",
      "25/03/09 00:21:49 WARN TaskSetManager: Lost task 2.0 in stage 136.0 (TID 190) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '2013-09-26T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2013-09-26T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 22 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o697.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 136.0 failed 1 times, most recent failure: Lost task 3.0 in stage 136.0 (TID 191) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2013-11-02T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '2013-11-02T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2013-11-02T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '2013-11-02T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mflatten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myyyy-MM-dd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_date_converted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morder_status\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morder_item_subtotal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_status IN (\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCOMPLETE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCLOSED\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morder_date_converted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myyyy-MM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morder_month\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morder_item_subtotal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRevenue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morder_month\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o697.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 136.0 failed 1 times, most recent failure: Lost task 3.0 in stage 136.0 (TID 191) (ziad-Lenovo-Legion-Y530-15ICH executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2013-11-02T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '2013-11-02T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2013-11-02T00:00:00.000+02:00' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '2013-11-02T00:00:00.000+02:00' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "flatten.select('customer_id','first_name',col(\"order_date\"),to_date(col(\"order_date\"),\"yyyy-MM-dd\").alias(\"order_date_converted\"),'order_status','order_item_subtotal'). \\\n",
    "filter(\"order_status IN ('COMPLETE','CLOSED')\"). \\\n",
    "groupBy(date_format('order_date_converted','yyyy-MM').alias('order_month')). \\\n",
    "agg(_sum('order_item_subtotal').alias('Revenue')). \\\n",
    "orderBy('order_month'). \\\n",
    "show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa3ce5-5650-4a59-988f-fd11643a2c5f",
   "metadata": {},
   "source": [
    "**load json data into mongoDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bd03a981-61c0-4280-b36c-09fdb7fed51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 01:03:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkToMongo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.2.1\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/ziad.demo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/ziad.demo\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "39f3d55f-340f-4778-899b-a861d9ea539e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o746.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjson_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o746.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "json_df.write.format(\"mongo\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2689719-cb0b-462e-ad8a-8153a3fb7a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92345a08-1d7c-44bf-9da6-c5bdc5b2bb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5512b2-e54d-44e7-b9be-c04c30f57ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d5554-9d77-4c5c-ba73-031c0811baf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bf2e4-6fc3-482d-9004-2699d3281f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
